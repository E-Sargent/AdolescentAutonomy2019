#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:26:21 2019

@author: erics
"""

import numpy as np
import pandas as pd
import statsmodels.api as sm
from HelperFrames import mappings, files, variables
from HelperFunctions import zscore, cleandupes, isnumeric


i = pd.Series('mcsid')
I = pd.Series('MCSID')
raw = {}

#Imports all datafiles as defined by the "files" DataFrame, for files which do not
#include duplicate entries for cohort members it simply imports the file with
#necessary columns defined by the "variables" DataFrame. For files which do contain
#duplicate cohort member entrys they are cleaned by the "cleandupes" function.
#All files are indexed under "MCSID", changing case as necessay
for name, file in files.iterrows():
    if file['dupes'] == False:
        if file['lower'] == True:
            raw[name] = pd.read_stata(file['dir']+name, columns = variables[variables['source'] == name]['dataname'].append(i), index_col = 'mcsid')
            raw[name].index.names = ['MCSID']
        else:
            raw[name] = pd.read_stata(file['dir']+name, columns = variables[variables['source'] == name]['dataname'].append(I), index_col = 'MCSID')
    elif file['dupes'] == True:
        df = pd.read_stata(file['dir']+name)
        df = cleandupes(df, file['conds'])
        raw[name] = df[variables[variables['source'] == name]['dataname']]

#Imports a pre-cleaned version of the time-use diaries
#If cleaning has not been completed runs the time_use_diaries file to complete this
try:
    mcs = pd.read_pickle('tud_weekday.pkl')
except FileNotFoundError:
    from TimeUseDiaries import summary
    mcs = summary.copy()

#Merges the various datafiles collected by raw dictionary into the main DataFrame, mcs
for df in raw.keys():
    mcs = mcs.join(raw[df])

#Renames columns of the mcs using a dictionary generated by the "variables" DataFrame
mcs = mcs.rename(index=str, columns = {var['dataname']:name for name, var in variables.iterrows()})

#Maps variables based on the "variables" DataFrame
for name, row in variables[variables['map'].isnull() == False].iterrows():
    mcs[name] = mcs[name].map(mappings[row['map']])

#Identifies variables which are labelled as outcomes
outcomes = variables[variables['outcome'] != 0].index

#Creates a list of rows which are missing outcomes and drops them
to_drop = [name for name,row in mcs[outcomes].applymap(isnumeric).iterrows() if False in row.values]
mcs = mcs.drop(to_drop)


#Translate categorical DataFrame into numeric type
mcs = mcs.apply((lambda x: pd.to_numeric(x, errors='coerce')))

#Add one to income so that taking logs doesn't divide by zero
logs = variables[variables['transformation'] == np.log].index
mcs[logs] = mcs[logs].apply(lambda x: x+1)


#Apply functions defined by "variables" DataFrame 
ops = variables[variables['transformation'].isnull() == False]['transformation']
for var, func in ops.iteritems():
    mcs[var] = func(mcs[var])
    
#Fill NaN with zeroes
mcs = mcs.fillna(0)

#------------------------------------------------------------------------------
#Creating new derived variables
#------------------------------------------------------------------------------
#Computing internalisnig and externalising scores for each sweep
mcs['internal3'] = zscore(mcs['n3e'] + mcs['n3p'],flip_sign=True)
mcs['external3'] = zscore(mcs['n3c'] + mcs['n3h'],flip_sign=True)
mcs['internal2'] = zscore(mcs['n2e'] + mcs['n2p'],flip_sign=True)
mcs['external2'] = zscore(mcs['n2c'] + mcs['n2h'],flip_sign=True)
mcs['internal1'] = zscore(mcs['n1e'] + mcs['n1p'],flip_sign=True)
mcs['external1'] = zscore(mcs['n1c'] + mcs['n1h'],flip_sign=True)

#Summing  third sweep investments
mcs['f3'] = zscore(mcs['f31']+mcs['f32']+mcs['f33'])
mcs['z3'] = zscore(mcs['z31']+mcs['z32']+mcs['z33'])
mcs['z3t'] = mcs['hr'] + mcs['exercise'] + mcs['creative']

#Sum of second sweep investments
mcs['z2'] = zscore(mcs['z21']+mcs['z22']+mcs['z23'])
mcs['f2'] = zscore(mcs['f21']+mcs['f22']+mcs['f23']+mcs['f24']+mcs['f25']+mcs['f26'])


#Creating instruments for t = 3 cognitive and non-cognitive ability
civ3 = sm.OLS(mcs['c3q'],sm.add_constant(mcs['c3v'])).fit()
mcs['c3p'] = civ3.predict()

niv3 = sm.OLS(mcs['internal3'],sm.add_constant(mcs[['external3','i3']])).fit()
mcs['n3p'] = niv3.predict()

#Creating instruments for t = 2 cognitive and non-cognitive ability
civ2 = sm.OLS(mcs['c2q'],sm.add_constant(mcs['c2v'])).fit()
mcs['c2p'] = civ2.predict()

niv2 = sm.OLS(mcs['internal2'],sm.add_constant(mcs[['external2','i2']])).fit()
mcs['n2p'] = niv2.predict()

#Creating instruments for t = 1 cognitive and non-cognitive ability
civ1 = sm.OLS(mcs['c1q'],sm.add_constant(mcs['c1v'])).fit()
mcs['c1p'] = civ1.predict()

niv1 = sm.OLS(mcs['internal1'],sm.add_constant(mcs[['external1','i1']])).fit()
mcs['n1p'] = niv1.predict()


#Save for later use
mcs.to_pickle('mcs.pkl')
















